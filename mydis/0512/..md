# 目標:調整學習率&entropy值
# 發現收斂結果策略單一 >>>> 需增加-保持多樣性

## 怎麼讓它保持一定多樣性？

1. 提高熵係數
  
   把 `self.entropy_coef` 调大，或在训练后期不要把它 `decay`，这样 `policy loss` 里就始终有一项在惩罚过低的熵

2. 策略噪声／ε-greedy
 
   即便是使用 `Categorical`，也可以在采样后有一个小概率 ε 随机选一个动作，保证永远有一点探索：
   ```
   if random.random() < eps:
       action = random.randrange(n_actions)
   else:
       action = dist.sample().item
   ```

3. 环境随机化
   
   * 每个 episode 随机初始化 B 船和 A 船的位置／航向。

   * 随机改变 done 条件（比如距离阈值），迫使 Manager/Worker 针对多种情况都得学习不同策略。

5. 学习率与网络容量

   * 如果 Learning Rate 太小，后期梯度极小，策略停滞不前；太大又会抖动。可尝试在训练中段稍微调高一点，也可以用 Learning Rate Scheduler。

   * 增加 LSTM 隐藏层大小或堆叠更多层，也能让网络有更大的“表征容量”去对付多样化场景。



# 結果  
![image](https://github.com/Yuu-Hsuan/CMO/blob/main/mydis/0512/0.png)


# 架構  
![image](https://github.com/Yuu-Hsuan/CMO/blob/main/mydis/0512/1.png)
